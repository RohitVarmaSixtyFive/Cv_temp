{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 PatchSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](plots_temp/1.png)\n",
    "\n",
    "- We can see that 4 is the sweetspot between 2 and 8 for slightly larger dimensions\n",
    "- 2 would work better for a larger dataset but it would create too many parameters for training but with a bottleneck of 50k dataset its pretty hard to train meaningfully\n",
    "- 8 on the other hand is too less of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 and 1.2.3\n",
    "\n",
    "- Hyperparameter exploration: with a bottle neck of 50k dataset we augument the dataset using Flip and pad_followed_by_crop to help generalize training more for differnt parts of images at different angles and stuff.\n",
    "\n",
    "![](plots_temp/2.png)\n",
    "\n",
    "- From this we can see that 2d-learned > 1d_learned because images and ones and sinusoidal performance is not that good for smaller models, sinusoidal is better for larger models(in the diagram 768 hidden dimension)\n",
    "- color jittering works very bad compared to rotations and crops. \n",
    "- The below two are for color jittering and flip + crop\n",
    "![](plots_temp/3.png)\n",
    "- larger layers generalized better. note this is with data augumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dino to 480x480 resized and visualized \n",
    "\n",
    "This is the img\n",
    "\n",
    "![](plots_1.4.1/img.png)\n",
    "\n",
    "![](plots_1.4.1/attn-head0.png)\n",
    "![](plots_1.4.1/attn-head1.png)\n",
    "\n",
    "these are first two layers\n",
    "\n",
    "![](plots_1.4.1/attn-head10.png)\n",
    "![](plots_1.4.1/attn-head11.png)\n",
    "\n",
    "these are last two have more aggregated meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cifar attention maps\n",
    "\n",
    "![](plots_1.4.1/attn-head0.png)\n",
    "![](plots_1.4.1/attn-head1.png)\n",
    "![](plots_1.4.1/attn-head2.png)\n",
    "![](plots_1.4.1/attn-head3.png)\n",
    "\n",
    "- these are for 4 heads, each head is supposed to concetrate on diff things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below are initial layer heads\n",
    "\n",
    "![](plots_1.4.2/layer_0_attn-head0.png)\n",
    "![](plots_1.4.2/layer_0_attn-head1.png)\n",
    "![](plots_1.4.2/layer_0_attn-head2.png)\n",
    "![](plots_1.4.2/layer_0_attn-head3.png)\n",
    "\n",
    "- This is last layer\n",
    "\n",
    "![](plots_1.4.2/layer_7_attn-head0.png)\n",
    "![](plots_1.4.2/layer_7_attn-head1.png)\n",
    "![](plots_1.4.2/layer_7_attn-head2.png)\n",
    "![](plots_1.4.2/layer_7_attn-head3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 rollout\n",
    "\n",
    "![](plots_1.4.3/attention_rollout_with_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 pos_embeddings\n",
    "![](plots_1.4.4/pos_embed_similarity.png)\n",
    "![](plots_1.4.4/pos_embed_grid_similarity.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
