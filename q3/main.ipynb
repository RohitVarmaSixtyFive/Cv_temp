{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c16edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    import clip\n",
    "    from io import BytesIO\n",
    "    import torch\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "    from torchvision.models import ResNet50_Weights\n",
    "    from torchvision import datasets, transforms\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt    \n",
    "    import numpy as np\n",
    "    from matplotlib import image as mpimg\n",
    "    import torch.nn.functional as F\n",
    "    import time\n",
    "    from tabulate import tabulate\n",
    "    import subprocess\n",
    "    import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0621a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I downloaded the synset_words.txt from https://github.com/torch/tutorials/blob/master/7_imagenet_classification/synset_words.txt\n",
    "\n",
    "[PROMPT] : Write a helper function to read this text file and print both the synset and its group's name\n",
    "\"\"\"\n",
    "\n",
    "def load_synset_words(synset_word_location):\n",
    "    \"\"\"\n",
    "    Just a helper function to read the synset ID and the names \n",
    "    \"\"\"\n",
    "    synset_to_class = {}\n",
    "\n",
    "    with open(synset_word_location, \"r\") as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            \n",
    "            if len(parts) == 2:\n",
    "                synset_to_class[parts[0]] = parts[1]\n",
    "            else:\n",
    "                print(f\"Skipping invalid line: {line.strip()}\")\n",
    "    \n",
    "    return synset_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc96a7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/interim/3/synset_words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m synset_word_location = \u001b[33m\"\u001b[39m\u001b[33m../../data/interim/3/synset_words.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m synset_words = \u001b[43mload_synset_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynset_word_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m count = \u001b[32m0\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m synset_id, name \u001b[38;5;129;01min\u001b[39;00m synset_words.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mload_synset_words\u001b[39m\u001b[34m(synset_word_location)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mJust a helper function to read the synset ID and the names \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m synset_to_class = {}\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msynset_word_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.strip():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../data/interim/3/synset_words.txt'"
     ]
    }
   ],
   "source": [
    "synset_word_location = \"../../data/interim/3/synset_words.txt\"\n",
    "synset_words = load_synset_words(synset_word_location)\n",
    "\n",
    "\n",
    "count = 0 \n",
    "\n",
    "for synset_id, name in synset_words.items():\n",
    "    print(f\"Synset ID: {synset_id}, Name: {name}\")\n",
    "    count = count + 1\n",
    "    \n",
    "    if(count>5):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67760549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of labels : {len(synset_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7cdaf5",
   "metadata": {},
   "source": [
    "`1. Inference using CLIP.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bbe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet_50_weight():\n",
    "    \"\"\"\n",
    "    This function loads the pretrained weights for ResNet-50 \n",
    "    # Ref : https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "    \"\"\"\n",
    "    \n",
    "    resnet50_imagenet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    resnet50_imagenet.eval()\n",
    "\n",
    "    return resnet50_imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_imagenet = load_resnet_50_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798bf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available models in CLIP are : {clip.available_models()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model):\n",
    "    \"\"\"\n",
    "    This function loads the CLIP and any of its available model \n",
    "    # Ref => https://github.com/openai/CLIP?tab=readme-ov-file\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model, preprocess = clip.load(model, device)\n",
    "\n",
    "    return model, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1052a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_RN50 , preprocess = load_clip_model(model=\"RN50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868710d",
   "metadata": {},
   "source": [
    "\n",
    "### Difference in the Visual Encoder\n",
    "\n",
    "![image.png](../../data/interim/3/markdown_space.png)\n",
    "\n",
    "\n",
    "CLIP has two encoders: one for the visual (either ViT or ResNet) and one for the text (using Transformers).\n",
    "\n",
    "The differences in the encoder architectures between CLIP and ResNet are as follows:\n",
    "\n",
    "1. **Input Resolution and Normalization**: \n",
    "  - Standard ImageNet ResNet-50 expects 224×224 images with specific normalization values.\n",
    "  - CLIP's vision encoder is designed to handle multiple input resolutions and uses different normalization constants.\n",
    "\n",
    "2. **Attention Pooling**: \n",
    "  - CLIP replaces the standard global average pooling used in ResNet-50 with an attention pooling (where we basically weight the features and then pool them) mechanism.\n",
    "  - This allows the model to focus on more relevant parts of the image when creating the final representation.\n",
    "\n",
    "3. **Modified Final Layer**: \n",
    "  - In standard ResNet-50, the final layer is a linear classifier that outputs logits for 1000 ImageNet classes.\n",
    "  - CLIP's vision encoder, on the other hand, outputs a normalized embedding vector (typically 1024 or 2048 dimensions) designed to align with the text embedding space so that the dot product can be computed \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af8ad8",
   "metadata": {},
   "source": [
    "`2. Setup data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880f446",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9a45374",
   "metadata": {},
   "source": [
    "`3. Setup zero-shot CLIP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb11f635",
   "metadata": {},
   "source": [
    "`3.1 Testing CLIP on imagenet dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b346ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(synset_words.keys())\n",
    "\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_clip(model, preprocess, device, image_path, class_labels):\n",
    "    \"\"\"\n",
    "    This function classifies an image with CLIP and returns the top-5 predictions.\n",
    "\n",
    "    The code is exactly from https://github.com/openai/CLIP?tab=readme-ov-file\n",
    "    \"\"\"\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {label}\") for label in class_labels]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze(0)\n",
    "    \n",
    "    values, indices = similarity.topk(5)\n",
    "    \n",
    "    predictions = [(class_labels[idx.item()], values[i].item()) for i, idx in enumerate(indices)]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I downloaded the mini imagenet dataset from https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000/discussion/284032\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dir = \"../../data/external/3/imagenet-mini/train\"\n",
    "\n",
    "imagenet_data = datasets.ImageFolder(root=train_dir, transform=preprocess)\n",
    "\n",
    "synset_ids = imagenet_data.classes  # these are like n017839 .. and not human readable -> so convert to human labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a45521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "human_readable_labels = []\n",
    "\n",
    "for synset_id in synset_ids:\n",
    "    if synset_id in synset_words:\n",
    "        human_readable_labels.append(synset_words[synset_id].split(\",\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01418bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(human_readable_labels[:5])\n",
    "\n",
    "print(f\"\\nNumber of labels = {len(human_readable_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc118298",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_IMAGES = 5 \n",
    "\n",
    "for idx in range(NUMBER_IMAGES):\n",
    "\n",
    "    synset_id = synset_ids[idx]  \n",
    "    synset_dir = os.path.join(train_dir, synset_id)  \n",
    "    \n",
    "    image_files = [f for f in os.listdir(synset_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    random_image_file = random.choice(image_files)\n",
    "    image_path = os.path.join(synset_dir, random_image_file)\n",
    "\n",
    "    true_label = human_readable_labels[idx]\n",
    "    \n",
    "    predictions = classify_with_clip(clip_RN50, preprocess, device, image_path, human_readable_labels)\n",
    "\n",
    "    print(f\"\\nTrue label: {true_label} (synset ID: {synset_id})\")\n",
    "\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off') \n",
    "    plt.title(f\"True: {true_label}\\nTop-5 Predictions\")\n",
    "    plt.show()\n",
    "\n",
    "    for label, score in predictions:\n",
    "        print(f\"Label: {label}, Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb535f60",
   "metadata": {},
   "source": [
    "`3.2 Testing ResNet50 pretrained on imagenet dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a standard pre-processing done for imagenet dataset and ref = https://stackoverflow.com/questions/67185623/image-net-preprocessing-using-torch-transforms\n",
    "\"\"\"\n",
    "\n",
    "resnet_preprocess = transforms.Compose([\n",
    "\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcc383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_resnet50(model, image_path, imagenet_labels , top_k=5):\n",
    "    \"\"\"\n",
    "    This function classifies an image with ResNet50 and returns the top-k predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = resnet_preprocess(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    \n",
    "    values, indices = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    predictions = [(imagenet_labels[idx.item()], values[i].item()) for i, idx in enumerate(indices)]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118dd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_imagenet = resnet50_imagenet.to(device)\n",
    "\n",
    "imagenet_labels = human_readable_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef63b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUMBER_IMAGES = 5\n",
    "\n",
    "for idx in range(NUMBER_IMAGES):\n",
    "\n",
    "    synset_id = synset_ids[idx]\n",
    "    synset_dir = os.path.join(train_dir, synset_id)\n",
    "\n",
    "    image_files = [f for f in os.listdir(synset_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    random_image_file = random.choice(image_files)\n",
    "    image_path = os.path.join(synset_dir, random_image_file)\n",
    "    \n",
    "    true_label = human_readable_labels[idx]\n",
    "    \n",
    "    resnet_predictions = classify_with_resnet50(resnet50_imagenet, image_path, imagenet_labels)\n",
    "\n",
    "    print(f\"\\nClassifying with ResNet50\")\n",
    "    print(f\"True label: {true_label} (synset ID: {synset_id})\")\n",
    "    \n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  \n",
    "    plt.title(f\"True: {true_label}\\nResNet50 Top-5 Predictions\")\n",
    "    plt.show()\n",
    "    \n",
    "    for label, score in resnet_predictions:\n",
    "        print(f\"Label: {label}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3609ac2d",
   "metadata": {},
   "source": [
    "`4. CLIP vs ImageNet pretraining.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(resnet50_model, clip_model, preprocess , image_path, imagenet_labels, device):\n",
    "    \"\"\"\n",
    "    Function to get predictions from both ResNet-50 and CLIP models.\n",
    "    \"\"\"\n",
    "\n",
    "    resnet_prediction = classify_with_resnet50(resnet50_model, image_path, imagenet_labels)\n",
    "\n",
    "    clip_prediction = classify_with_clip(clip_model, preprocess, device, image_path, imagenet_labels)\n",
    "    \n",
    "    return resnet_prediction, clip_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642916e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class 1 : Fruits\n",
    "\"\"\"\n",
    "\n",
    "folder_path = \"../../data/interim/3/clip-yes-resnet-no/1\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {filename}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        resnet_prediction, clip_prediction = get_predictions(resnet50_imagenet, clip_RN50, preprocess, image_path, imagenet_labels, device)\n",
    "\n",
    "        print(f\"== Predictions for: {filename} ==\")\n",
    "\n",
    "        for label, score in resnet_prediction:\n",
    "            print(f\"ResNet Label: {label}, Score: {score:.4f}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        for label, score in clip_prediction:\n",
    "            print(f\"CLIP Label: {label}, Score: {score:.4f}\")\n",
    "      \n",
    "\n",
    "        print(\"\\n\" + \"=\"*100 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_to_label = dict(zip(synset_ids, human_readable_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# These will store the mismatched image paths\n",
    "in_resnet_not_clip = []\n",
    "not_resnet_in_clip = []\n",
    "\n",
    "# Root directory with synset-style folders\n",
    "root_folder = \"../../data/external/3/imagenet-mini/val\"\n",
    "\n",
    "# Get list of folders first\n",
    "synset_folders = [f for f in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, f))]\n",
    "\n",
    "for synset_id in tqdm(synset_folders, desc=\"Processing synsets\"):\n",
    "\n",
    "    class_path = os.path.join(root_folder, synset_id)\n",
    "\n",
    "    # Get the human-readable label\n",
    "    true_label = synset_to_label.get(synset_id, \"\").lower()\n",
    "\n",
    "\n",
    "    if not true_label:\n",
    "        continue  # Skip if mapping not found\n",
    "\n",
    "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for filename in tqdm(image_files, leave=False, desc=f\"{synset_id}\", \n",
    "                         postfix=lambda: {\n",
    "                             \"in_resnet_not_clip\": len(in_resnet_not_clip),\n",
    "                             \"not_resnet_in_clip\": len(not_resnet_in_clip)\n",
    "                         }):\n",
    "        image_path = os.path.join(class_path, filename)\n",
    "\n",
    "        # Run predictions\n",
    "        resnet_pred, clip_pred = get_predictions(\n",
    "            resnet50_imagenet, clip_RN50, preprocess, image_path, imagenet_labels, device\n",
    "        )\n",
    "\n",
    "        # Get just the labels\n",
    "        resnet_labels = [label.lower() for label, _ in resnet_pred]\n",
    "        clip_labels = [label.lower() for label, _ in clip_pred]\n",
    "\n",
    "\n",
    "        in_resnet = any(true_label in pred_label for pred_label in resnet_labels)\n",
    "        in_clip = any(true_label in pred_label for pred_label in clip_labels)\n",
    "\n",
    "        if in_resnet and not in_clip:\n",
    "            in_resnet_not_clip.append(image_path)\n",
    "            print(f\"[Updated] in_resnet_not_clip: {len(in_resnet_not_clip)}\")\n",
    "        elif in_clip and not in_resnet:\n",
    "            not_resnet_in_clip.append(image_path)\n",
    "            print(f\"[Updated] not_resnet_in_clip: {len(not_resnet_in_clip)}\")\n",
    "\n",
    "\n",
    "# Save results to file\n",
    "with open(\"in_resnet_not_clip.txt\", \"w\") as f:\n",
    "    for path in in_resnet_not_clip:\n",
    "        f.write(f\"{path}\\n\")\n",
    "\n",
    "with open(\"not_resnet_in_clip.txt\", \"w\") as f:\n",
    "    for path in not_resnet_in_clip:\n",
    "        f.write(f\"{path}\\n\")\n",
    "\n",
    "print(f\"✅ Done! {len(in_resnet_not_clip)} in ResNet but not CLIP\")\n",
    "print(f\"✅ Done! {len(not_resnet_in_clip)} in CLIP but not ResNet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise KeyboardInterrupt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = human_readable_labels[idx]\n",
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "in_resnet_not_clip = []\n",
    "not_resnet_in_clip = []\n",
    "\n",
    "root_folder = \"../../data/external/3/imagenet-mini/train/\"\n",
    "\n",
    "for class_name in os.listdir(root_folder):\n",
    "\n",
    "    class_path = os.path.join(root_folder, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(class_path):\n",
    "        \n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(class_path, filename)\n",
    "\n",
    "            # Load image\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Get top-5 predictions from both models\n",
    "            resnet_pred, clip_pred = get_predictions(\n",
    "                resnet50_imagenet, clip_RN50, preprocess, image_path, imagenet_labels, device, topk=5\n",
    "            )\n",
    "\n",
    "            # Extract just the labels\n",
    "            resnet_labels = [label.lower() for label, _ in resnet_pred]\n",
    "            clip_labels = [label.lower() for label, _ in clip_pred]\n",
    "\n",
    "            true_label = class_name.lower()\n",
    "\n",
    "            in_resnet = any(true_label in pred_label for pred_label in resnet_labels)\n",
    "            in_clip = any(true_label in pred_label for pred_label in clip_labels)\n",
    "\n",
    "            if in_resnet and not in_clip:\n",
    "                in_resnet_not_clip.append(image_path)\n",
    "            elif in_clip and not in_resnet:\n",
    "                not_resnet_in_clip.append(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c86482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class 2\n",
    "\"\"\"\n",
    "\n",
    "folder_path = \"../../data/interim/3/clip-yes-resnet-no/2\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {filename}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        resnet_prediction, clip_prediction = get_predictions(resnet50_imagenet, clip_RN50, preprocess, image_path, imagenet_labels, device)\n",
    "\n",
    "        print(f\"== Predictions for: {filename} ==\")\n",
    "\n",
    "        for label, score in resnet_prediction:\n",
    "            print(f\"ResNet Label: {label}, Score: {score:.4f}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        for label, score in clip_prediction:\n",
    "            print(f\"CLIP Label: {label}, Score: {score:.4f}\")\n",
    "      \n",
    "\n",
    "        print(\"\\n\" + \"=\"*100 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f9b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4759343",
   "metadata": {},
   "source": [
    "`5. FP16`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1366040",
   "metadata": {},
   "source": [
    "`5.1 Estimating the wall-clock time taken by fp16 and fp32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89844139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To understand what wall - clock time is : https://stackoverflow.com/questions/7335920/what-specifically-are-wall-clock-time-user-cpu-time-and-system-cpu-time-in-uni\n",
    "\n",
    "Briefly : The wall-clock time is not the number of seconds that the process has spent on the CPU; it is the elapsed time, \n",
    "including time spent waiting for its turn on the CPU (while other processes get to run).\n",
    "\"\"\"\n",
    "\n",
    "def measure_the_wall_clock_inference_time(model, image, RUNS=100):\n",
    "    \"\"\"\n",
    "    This function computes the `wall-clock` time taken to encode an image and we consider\n",
    "    total of 100 runs ad default\n",
    "    \"\"\"\n",
    "\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(RUNS):\n",
    "\n",
    "            start = time.time()\n",
    "            _ = model.encode_image(image)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            times.append(time.time() - start)\n",
    "\n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_RN50_fp32 , preprocess = load_clip_model(model=\"RN50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea530e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../../data/external/3/imagenet-mini/train/n01443537/n01443537_1298.JPEG\"\n",
    "\n",
    "source_image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "fp32_mean, fp32_std = measure_the_wall_clock_inference_time(model = clip_RN50_fp32, \n",
    "                                                            image=source_image, RUNS=1000)\n",
    "\n",
    "print(f\"FP32: {fp32_mean:.6f}s ± {fp32_std:.6f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To convert the model to half precision : https://discuss.pytorch.org/t/converting-model-into-16-points-precisoin-float16-instead-of-32/102622/12\n",
    "\n",
    "TLDR : model.half() will transform all parameters and buffers to float16 and NOTE to convert also the image since its an input (also mentioned in the above ref)\n",
    "\"\"\"\n",
    "\n",
    "clip_rn50_fp16 = clip_RN50_fp32.half()\n",
    "source_image_fp16 = source_image.half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32025a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_mean, fp16_std = measure_the_wall_clock_inference_time(model = clip_rn50_fp16,\n",
    "                                                             image= source_image_fp16 ,\n",
    "                                                             RUNS=100)\n",
    "\n",
    "print(f\"FP16: {fp16_mean:.6f}s ± {fp16_std:.6f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = fp32_mean / fp16_mean\n",
    "\n",
    "table = [\n",
    "    [\"Precision\", \"Mean Time (s)\", \"Std Dev (s)\"],\n",
    "    [\"FP32\", f\"{fp32_mean:.6f}\", f\"{fp32_std:.6f}\"],\n",
    "    [\"FP16\", f\"{fp16_mean:.6f}\", f\"{fp16_std:.6f}\"]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "\n",
    "print(f\"\\nSpeedup (FP32 / FP16): {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78587cf2",
   "metadata": {},
   "source": [
    "`5.2 Probabilities computed by fp16 and fp32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_clip_for_prob_check(model, preprocess, image_path, class_labels, use_fp16_for_image=False):\n",
    "    \"\"\"\n",
    "    Classify an image with CLIP and return probabilities for all classes.\n",
    "    If use_fp16_for_image is True, image encoding is done in FP16.\n",
    "    \"\"\"\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {label}\") for label in class_labels]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_fp16_for_image:\n",
    "            image_input = image_input.half()\n",
    "            image_features = model.encode_image(image_input)\n",
    "        else:\n",
    "            image_features = model.encode_image(image_input)\n",
    "\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze(0)\n",
    "\n",
    "    return similarity.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e053f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classification_outputs(model_fp32, model_fp16, preprocess):\n",
    "    \"\"\"\n",
    "    This function compares the classification outputs between FP32 and FP16 models for 5 images.\n",
    "    Prints a detailed, nicely formatted report with tabulated summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    NUM_IMAGES = 5\n",
    "    results = []\n",
    "\n",
    "    selected_indices = random.sample(range(len(synset_ids)), NUM_IMAGES)\n",
    "\n",
    "    for idx in selected_indices:\n",
    "        synset_id = synset_ids[idx]\n",
    "        synset_dir = os.path.join(train_dir, synset_id)\n",
    "\n",
    "        image_files = [f for f in os.listdir(synset_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    " \n",
    " \n",
    "        image_file = random.choice(image_files)\n",
    "        image_path = os.path.join(synset_dir, image_file)\n",
    "        true_label = human_readable_labels[idx]\n",
    "\n",
    "        # FP32\n",
    "        model_fp32 = model_fp32.float()\n",
    "        probs_fp32 = classify_with_clip_for_prob_check(model_fp32, preprocess, image_path, human_readable_labels)\n",
    "\n",
    "        # FP16\n",
    "        probs_fp16 = classify_with_clip_for_prob_check(model_fp16, preprocess, image_path, human_readable_labels, use_fp16_for_image=True)\n",
    "\n",
    "        abs_diff = np.abs(probs_fp32 - probs_fp16).max()\n",
    "        rel_diff = abs_diff / np.maximum(probs_fp32.max(), 1e-10)\n",
    "\n",
    "        top5_indices_fp32 = np.argsort(probs_fp32)[-5:][::-1]\n",
    "        top5_indices_fp16 = np.argsort(probs_fp16)[-5:][::-1]\n",
    "\n",
    "        top1_match = top5_indices_fp32[0] == top5_indices_fp16[0]\n",
    "        top5_overlap = len(set(top5_indices_fp32).intersection(set(top5_indices_fp16)))\n",
    "\n",
    "        results.append({\n",
    "            'Class': true_label,\n",
    "            'Image': os.path.basename(image_path),\n",
    "            'Max Abs Diff': abs_diff,\n",
    "            'Relative Diff': rel_diff,\n",
    "            'Top-1 Match': top1_match,\n",
    "            'Top-5 Overlap': top5_overlap\n",
    "        })\n",
    "\n",
    "        # [PROMPT] : Write a helper function to plot the probabilities and the predictions and also tabulate the final results \n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Image: {os.path.basename(image_path)}\")\n",
    "        print(f\"True Class: {true_label}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        print(\"Top-5 FP32 Predictions:\")\n",
    "        for i in top5_indices_fp32:\n",
    "            print(f\"  {human_readable_labels[i]:<25} {probs_fp32[i]:.6f}\")\n",
    "        print(\"Top-5 FP16 Predictions:\")\n",
    "        for i in top5_indices_fp16:\n",
    "            print(f\"  {human_readable_labels[i]:<25} {probs_fp16[i]:.6f}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        print(f\"Top-1 Match: {top1_match}\")\n",
    "        print(f\"Top-5 Overlap: {top5_overlap}/5\")\n",
    "        print(f\"Max Abs Difference: {abs_diff:.6f}\")\n",
    "        print(f\"Relative Difference: {rel_diff:.6f}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        img = plt.imread(image_path)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True class: {true_label}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sorted_indices = np.argsort(np.abs(probs_fp32 - probs_fp16))[-10:]\n",
    "        labels = [human_readable_labels[i][:20] for i in sorted_indices]\n",
    "        fp32_values = [probs_fp32[i] for i in sorted_indices]\n",
    "        fp16_values = [probs_fp16[i] for i in sorted_indices]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        plt.bar(x - width/2, fp32_values, width, label='FP32')\n",
    "        plt.bar(x + width/2, fp16_values, width, label='FP16')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.title('Top Probability Differences: FP32 vs FP16')\n",
    "        plt.xticks(x, labels, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\\n Summary of Differences Between FP32 and FP16 Outputs:\\n\")\n",
    "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40044824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = compare_classification_outputs(clip_RN50_fp32, clip_rn50_fp16, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb3d3f",
   "metadata": {},
   "source": [
    "#### Why Is There No Significant Difference Between FP32 and FP16 Outputs?\n",
    "\n",
    "The minimal difference between FP32 and FP16 outputs can be attributed to the following technical reasons:\n",
    "\n",
    " 1. **Robustness of Neural Networks to Quantization**\n",
    "- Neural networks, especially large pre-trained models like CLIP or ResNet, are **inherently tolerant to small perturbations** in weights and activations.\n",
    "- Reducing precision from 32-bit to 16-bit introduces only minor noise, which the model is usually able to **absorb without affecting output rankings**.\n",
    "\n",
    "2. **Normalization Layers Minimize Precision Sensitivity**\n",
    "- Models often use **LayerNorm or BatchNorm**, which rescale activations and help **stabilize outputs across different precisions**.\n",
    "- These layers reduce the risk of large value ranges that could be affected by FP16's lower dynamic range.\n",
    "\n",
    "\n",
    "3. **Well-Calibrated Probabilities**\n",
    "- For tasks like image classification, final softmax probabilities tend to **saturate near 1 for the correct class** and decay quickly for others.\n",
    "- This leads to **large margins** between correct and incorrect predictions, making them **insensitive to small probability changes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef5cb4",
   "metadata": {},
   "source": [
    "`5.3 Memory Usage - forward pass `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2402047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    \"\"\"\n",
    "    This function returns the current GPU Usage\n",
    "    \"\"\"\n",
    "    \n",
    "    result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'])\n",
    "    return int(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def profile_model(model_type, model, preprocess, image_path, num_runs=100):\n",
    "    \n",
    "    print(f\"\\n--- Profiling {model_type} model with {num_runs} runs ---\")\n",
    "    \n",
    "    # Clear cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Get baseline memory usage\n",
    "    baseline = get_gpu_memory_usage()\n",
    "    print(f\"Baseline memory usage: {baseline} MB\")\n",
    "    \n",
    "    # Load image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(\"cuda\")\n",
    "    if model_type == \"FP16\":\n",
    "        image = image.half()\n",
    "    \n",
    "    # Get memory after loading image\n",
    "    after_image_load = get_gpu_memory_usage()\n",
    "    print(f\"Memory after loading image: {after_image_load} MB (+ {after_image_load - baseline} MB)\")\n",
    "    \n",
    "    # Perform forward passes\n",
    "    forward_times = []\n",
    "    max_memory = after_image_load\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            image_features = model.encode_image(image)\n",
    "            torch.cuda.synchronize()  # Make sure GPU operations are completed\n",
    "            end_time = time.time()\n",
    "            \n",
    "        forward_times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "        \n",
    "        # Check memory after each forward pass\n",
    "        current_memory = get_gpu_memory_usage()\n",
    "        max_memory = max(max_memory, current_memory)\n",
    "        \n",
    "        # Print progress for every 10 runs\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Completed {i + 1}/{num_runs} runs. Current memory: {current_memory} MB\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_time = np.mean(forward_times)\n",
    "    std_time = np.std(forward_times)\n",
    "    \n",
    "    # Get final memory usage\n",
    "    final_memory = get_gpu_memory_usage()\n",
    "    memory_increase = max_memory - after_image_load\n",
    "    \n",
    "    print(f\"Max memory after {num_runs} forward passes: {max_memory} MB (+ {memory_increase} MB from image load)\")\n",
    "    print(f\"Final memory after {num_runs} forward passes: {final_memory} MB\")\n",
    "    print(f\"Average forward pass time: {avg_time:.2f} ms (std: {std_time:.2f} ms)\")\n",
    "    \n",
    "    torch_allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # Convert to MB\n",
    "    torch_reserved = torch.cuda.memory_reserved() / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"PyTorch reported memory - Allocated: {torch_allocated:.2f} MB, Reserved: {torch_reserved:.2f} MB\")\n",
    "    \n",
    "    return {\n",
    "        \"max_memory_increase\": max_memory - baseline,\n",
    "        \"nvidia_smi_increase\": memory_increase,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"std_time\": std_time,\n",
    "        \"torch_allocated\": torch_allocated,\n",
    "        \"torch_reserved\": torch_reserved\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Main execution\n",
    "model_name = \"RN50\"  \n",
    "\n",
    "model_fp32, preprocess = clip.load(model_name, device=\"cuda\")\n",
    "fp32_results = profile_model(\"FP32\", model_fp32, preprocess, image_path)\n",
    "\n",
    "del model_fp32\n",
    "torch.cuda.empty_cache()\n",
    "time.sleep(3)  \n",
    "\n",
    "model_fp16, _ = clip.load(model_name, device=\"cuda\")\n",
    "model_fp16 = model_fp16.half()\n",
    "fp16_results = profile_model(\"FP16\", model_fp16, preprocess, image_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED COMPARISON BETWEEN FP32 AND FP16\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Max Memory Increase (nvidia-smi):\")\n",
    "print(f\"  FP32: {fp32_results['max_memory_increase']} MB\")\n",
    "print(f\"  FP16: {fp16_results['max_memory_increase']} MB\")\n",
    "print(f\"  Savings: {fp32_results['max_memory_increase'] - fp16_results['max_memory_increase']} MB\")\n",
    "print(f\"  Reduction: {(1 - fp16_results['max_memory_increase']/fp32_results['max_memory_increase'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nPyTorch Allocated Memory:\")\n",
    "print(f\"  FP32: {fp32_results['torch_allocated']:.2f} MB\")\n",
    "print(f\"  FP16: {fp16_results['torch_allocated']:.2f} MB\")\n",
    "print(f\"  Savings: {fp32_results['torch_allocated'] - fp16_results['torch_allocated']:.2f} MB\")\n",
    "print(f\"  Reduction: {(1 - fp16_results['torch_allocated']/fp32_results['torch_allocated'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nPyTorch Reserved Memory:\")\n",
    "print(f\"  FP32: {fp32_results['torch_reserved']:.2f} MB\")\n",
    "print(f\"  FP16: {fp16_results['torch_reserved']:.2f} MB\")\n",
    "print(f\"  Savings: {fp32_results['torch_reserved'] - fp16_results['torch_reserved']:.2f} MB\")\n",
    "print(f\"  Reduction: {(1 - fp16_results['torch_reserved']/fp32_results['torch_reserved'])*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def profile_model_with_pytorch(model_type, model, preprocess, image_path, num_runs=100):\n",
    "    \"\"\"\n",
    "    Profiles the memory usage and forward pass time for a model (FP32 or FP16) using PyTorch's built-in profiler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clear cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Load image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(\"cuda\")\n",
    "    if model_type == \"FP16\":\n",
    "        image = image.half()  # Convert to FP16\n",
    "\n",
    "    # Initialize PyTorch Profiler\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU, \n",
    "            torch.profiler.ProfilerActivity.CUDA\n",
    "        ],\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as profiler:\n",
    "        for _ in range(num_runs):\n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                model.encode_image(image)\n",
    "                torch.cuda.synchronize()  # Ensure GPU operations are completed\n",
    "                end_time = time.time()\n",
    "\n",
    "                # Record memory usage and time\n",
    "                profiler.step()  # Step through the profiler for each iteration\n",
    "            forward_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "            print(f\"Forward pass time: {forward_time:.2f} ms\")\n",
    "    \n",
    "    # You can visualize the profile using TensorBoard\n",
    "    # To do this, open the TensorBoard interface with the following:\n",
    "    # tensorboard --logdir=./log\n",
    "\n",
    "# Example usage\n",
    "profile_model_with_pytorch(\"FP32\", clip_RN50_fp32, preprocess, image_path)\n",
    "profile_model_with_pytorch(\"FP16\", clip_rn50_fp16, preprocess, image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
